{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import requests\n",
    "import sys\n",
    "sys.path.append('D:/Deployment/airflowSpark')\n",
    "from jobs.common.spark_common import initiate_spark,terminate_spark, get_circle_data , get_process_sensor, get_data_from_api,reading_and_validate_data,train_test_split\n",
    "# from jobs.common.spark_common import initiate_spark,terminate_spark, get_circle_data , get_process_sensor, get_data_from_api,reading_and_validate_data,train_test_split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import joblib\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LocalSparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x182199e7090>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = initiate_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16430\n",
      "18322\n",
      "17211\n",
      "18589\n",
      "20236\n",
      "14214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "circle_list = get_circle_data()\n",
    "# data = get_process_sensor(circle_list[0]['id'])\n",
    "data= []\n",
    "for id in range(len(circle_list)):\n",
    "    data_complete  = get_process_sensor(circle_list[id]['id'])\n",
    "    data.extend(data_complete )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/Deployment/airflowSpark')\n",
    "\n",
    "schema_file_path = \"schema.yaml\"\n",
    "\n",
    "# Validate the data against the schema\n",
    "validated_df = reading_and_validate_data(spark, data, schema_file_path)\n",
    "\n",
    "# Check if validation was successful\n",
    "if validated_df is not None:\n",
    "    df = validated_df\n",
    "    # df = validated_df.select(\"creation_time\",\"sensor_id\",\"consumed_unit\")\n",
    "    # print(df.count())\n",
    "\n",
    "else:\n",
    "    print(\"Data validation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187370"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"sensor_id\", outputCol=\"encoded_sensor_id\")\n",
    "encoded_df = indexer.fit(df).transform(df)\n",
    "numbers_to_partitions = encoded_df.select(\"encoded_sensor_id\").distinct().count()\n",
    "print(encoded_df.select(\"sensor_id\").distinct().count())\n",
    "print(numbers_to_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encoded_df.drop(\"sensor_id\")\n",
    "num = df.rdd.getNumPartitions()\n",
    "print(\"number of partion before:\",num)\n",
    "repartitoned_df = df.repartition(numbers_to_partitions,['sensor_id'])\n",
    "num1 = repartitoned_df.rdd.getNumPartitions()\n",
    "print(\"number of partion after:\",num1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(key,pdf):\n",
    "    total_rows = len(pdf)\n",
    "    train_rows = int(0.8 * total_rows)\n",
    "    pdf = pdf.sort_values(by=\"creation_time\")\n",
    "    train_df = pdf.iloc[:train_rows]\n",
    "    FEATURES = ['is_holiday','temperature_2m', 'relative_humidity_2m', 'apparent_temperature','precipitation', 'wind_speed_10m','wind_speed_100m', 'lag1', 'lag2','lag3', 'lag4', 'lag5', 'day', 'hour', 'month', 'dayofweek', 'quarter','dayofyear', \"encoded_sensor_id\",'weekofyear', 'year']\n",
    "    TARGET = ['consumed_unit']\n",
    "    \n",
    "    X_train = train_df[FEATURES]\n",
    "    y_train = train_df[TARGET]\n",
    "\n",
    "    # from xgboost import XGBRegressor\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7],\n",
    "    'reg_alpha':  [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_model,\n",
    "                                        param_distributions=param_grid,\n",
    "                                        n_iter=10,\n",
    "                                        scoring='neg_mean_squared_error',\n",
    "                                        cv=5,\n",
    "                                        # verbose=1,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=45)\n",
    "\n",
    "    # Fit the RandomizedSearchCV to the data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    # print(f\"Best Parameters for sensor {i}: {best_params}\")\n",
    "\n",
    "    # Train the model with the best parameters\n",
    "    best_xgb_model = XGBRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                reg_alpha=best_params['reg_alpha'],\n",
    "                                reg_lambda=0.01,\n",
    "                                )\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "    model_filename = f\"xgb_model_sensor_{key}.joblib\"\n",
    "    joblib.dump(xgb_model, model_filename)\n",
    "    return X_train\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = StructType([\n",
    "            StructField(\"temperature_2m\", FloatType(), True),\n",
    "            StructField(\"relative_humidity_2m\", IntegerType(), True),\n",
    "            StructField(\"apparent_temperature\", FloatType(), True),\n",
    "            StructField(\"precipitation\", FloatType(), True),\n",
    "            StructField(\"wind_speed_10m\", FloatType(), True),\n",
    "            StructField(\"wind_speed_100m\", FloatType(), True),\n",
    "            StructField(\"is_holiday\", IntegerType(), True),\n",
    "            StructField(\"lag1\", FloatType(), True),\n",
    "            StructField(\"lag2\", FloatType(), True),\n",
    "            StructField(\"lag3\", FloatType(), True),\n",
    "            StructField(\"lag4\", FloatType(), True),\n",
    "            StructField(\"lag5\", FloatType(), True),\n",
    "            StructField(\"day\", IntegerType(), True),\n",
    "            StructField(\"hour\", IntegerType(), True),\n",
    "            StructField(\"month\", IntegerType(), True),\n",
    "            StructField(\"dayofweek\", IntegerType(), True),\n",
    "            StructField(\"quarter\", IntegerType(), True),\n",
    "            StructField(\"dayofyear\", IntegerType(), True),\n",
    "            StructField(\"encoded_sensor_id\", FloatType(), True),\n",
    "            StructField(\"weekofyear\", IntegerType(), True),\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = repartitoned_df.groupBy(\"encoded_sensor_id\").applyInPandas(split_train_test,schema=result_schema)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder to the 'sensor_id' column and transform it\n",
    "pdf['encoded_sensor_id'] = label_encoder.fit_transform(pdf['sensor_id'])\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "encoded_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pdf.groupby(['encoded_sensor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "model_dict = {}\n",
    "for id , data in grouped_df:\n",
    "    print(id[0])\n",
    "    df = data.copy()\n",
    "    model = split_train_test(pdf=df)\n",
    "    print(len(df))\n",
    "    model_dict[id[0]] = model\n",
    "    i+=1\n",
    "    if i>3:\n",
    "        break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(pdf):\n",
    "    total_rows = len(pdf)\n",
    "    train_rows = int(0.8 * total_rows)\n",
    "    pdf = pdf.sort_values(by=\"creation_time\")\n",
    "    train_df = pdf.iloc[:train_rows]\n",
    "    FEATURES = ['is_holiday','temperature_2m', 'relative_humidity_2m', 'apparent_temperature','precipitation', 'wind_speed_10m','wind_speed_100m', 'lag1', 'lag2','lag3', 'lag4', 'lag5', 'day', 'hour', 'month', 'dayofweek', 'quarter','dayofyear', \"encoded_sensor_id\",'weekofyear', 'year']\n",
    "    TARGET = ['consumed_unit']\n",
    "    \n",
    "    X_train = train_df[FEATURES]\n",
    "    y_train = train_df[TARGET]\n",
    "\n",
    "    # from xgboost import XGBRegressor\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7],\n",
    "    'reg_alpha':  [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_model,\n",
    "                                        param_distributions=param_grid,\n",
    "                                        n_iter=10,\n",
    "                                        scoring='neg_mean_squared_error',\n",
    "                                        cv=5,\n",
    "                                        # verbose=1,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=45)\n",
    "\n",
    "    # Fit the RandomizedSearchCV to the data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    # print(f\"Best Parameters for sensor {i}: {best_params}\")\n",
    "\n",
    "    # Train the model with the best parameters\n",
    "    best_xgb_model = XGBRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                reg_alpha=best_params['reg_alpha'],\n",
    "                                reg_lambda=0.01,\n",
    "                                )\n",
    "    model = best_xgb_model.fit(X_train, y_train)\n",
    "    return model\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f\"model.joblib\"\n",
    "joblib.dump(model_dict, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df.rdd.getNumPartitions()\n",
    "print(\"number of partion before:\",num)\n",
    "# df_repartitoned = df.repartition(\"sensor_id\")\n",
    "# num2 = df_repartitoned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_part = spark.sql(sql).repartition(spark.sparkContext.defaultParrallelism,['sensor_id']).cache()\n",
    "store_part = spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['sensor_id']).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_part.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType,IntegerType,StringType,TimestampType,StructType,StructField\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"temperature_2m\", FloatType(), True),\n",
    "    StructField(\"relative_humidity_2m\", IntegerType(), True),\n",
    "    StructField(\"apparent_temperature\", FloatType(), True),\n",
    "    StructField(\"precipitation\", FloatType(), True),\n",
    "    StructField(\"wind_speed_10m\", FloatType(), True),\n",
    "    StructField(\"wind_speed_100m\", FloatType(), True),\n",
    "    StructField(\"creation_time\", TimestampType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True),\n",
    "    StructField(\"is_holiday\", IntegerType(), True),\n",
    "    StructField(\"consumed_unit\", FloatType(), True),\n",
    "    StructField(\"lag1\", FloatType(), True),\n",
    "    StructField(\"lag2\", FloatType(), True),\n",
    "    StructField(\"lag3\", FloatType(), True),\n",
    "    StructField(\"lag4\", FloatType(), True),\n",
    "    StructField(\"lag5\", FloatType(), True),\n",
    "    StructField(\"lag6\", FloatType(), True),\n",
    "    StructField(\"lag7\", FloatType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"dayofweek\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "    StructField(\"dayofyear\", IntegerType(), True),\n",
    "    StructField(\"weekofyear\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# @pandas_udf(result_schema,PandasUDFType.GROUPED_MAP)\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "# def load_forecast(senor_data):\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "        print(\"hello\")\n",
    "        # total_rows = df.count()\n",
    "\n",
    "        # test_rows = int(0.2 * total_rows)\n",
    "        # train_rows = max(0, total_rows - test_rows)\n",
    "        # test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        # train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        # return [(train,test)]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# import pandas as pd\n",
    "# @pandas_udf(result_schema,PandasUDFType.GROUPED_MAP)\n",
    "# def train_test_split(df):\n",
    "#     try:\n",
    "#         print(\"hello\")\n",
    "#         return pd.DataFrame({\"result\": [df]})  # Example return, adjust as needed\n",
    "#     except Exception as e:\n",
    "#         print(\"error in splitting data:\", e)\n",
    "\n",
    "result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "# List of departments\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "\n",
    "# Randomly choose departments for each person\n",
    "data = [(name, random.choice(departments)) for name in names]\n",
    "\n",
    "# Extend the data to have some repetitive departments\n",
    "data_extended = data + data[:3]  # Repeat the first three departments\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data_extended, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitoned = df.repartition(\"department\")\n",
    "print(df_repartitoned.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(people):\n",
    "#     for person in people:\n",
    "#         print(person.Name)\n",
    "# df_repartitoned.foreachPartition(f)\n",
    "\n",
    "def f(iterator):\n",
    "    for person in iterator:\n",
    "        print(person.Name)\n",
    "\n",
    "# Apply the function to each partition\n",
    "df_repartitoned.foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    names = [person.Name for person in iterator]\n",
    "    print(names)\n",
    "\n",
    "# Apply the function to each partition and collect the results to the driver\n",
    "df_repartitoned.foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "data = [(name, random.choice(departments)) for name in names]\n",
    "df = spark.createDataFrame(data_extended, schema)\n",
    "# df.show()\n",
    "\n",
    "# Repartition the DataFrame based on the \"Department\" column\n",
    "df_repartitioned = df.repartition(\"Department\")\n",
    "print(df_repartitioned.rdd.getNumPartitions())\n",
    "# Print names within each partition using collect() and a loop\n",
    "# if df_repartitioned.rdd.isEmpty():\n",
    "#     print(\"DataFrame is empty\")\n",
    "# else:\n",
    "#     # Get names from each partition using collect()\n",
    "#     partitions = df_repartitioned.rdd.collect()\n",
    "\n",
    "#     # Loop through partitions and print names\n",
    "#     for partition in partitions:\n",
    "#         partition.show()\n",
    "#         names = [person.Name for person in partition]\n",
    "#         print(f\"Names in partition: {names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consumption_df = df.groupBy(\"sensor_id\").agg(sum(\"consumed_unit\").alias(\"total_consumption\"))\n",
    "total_consumption_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned = df.repartition(\"sensor_id\")\n",
    "# Define a UDF to encapsulate your calculations\n",
    "def analyze_timeseries(data):\n",
    "  # Your time series analysis logic goes here\n",
    "  # Example: Calculate mean consumption\n",
    "  mean_consumption = data.select(\"consumed_unit\").mean()  # Access the DataFrame within the UDF\n",
    "  return mean_consumption\n",
    "\n",
    "analyze_timeseries_udf = spark.udf.register(\"analyze_timeseries\", analyze_timeseries)\n",
    "\n",
    "# Apply the UDF to each partition (sensor)\n",
    "analyzed_df = df_repartitioned.withColumn(\"analysis_result\", analyze_timeseries_udf(col(\"consumed_unit\")))\n",
    "analyzed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "partitioned_df = df.repartition(\"sensor_id\") \n",
    "\n",
    "# Define a UDF to encapsulate calculations (using PySpark functions for clarity)\n",
    "def analyze_timeseries(data):\n",
    "  print(data)\n",
    "  mean_consumption = F.mean(data[\"consumed_unit\"])\n",
    "  return mean_consumption\n",
    "\n",
    "analyze_timeseries_udf = spark.udf.register(\"analyze_timeseries\", analyze_timeseries)\n",
    "\n",
    "# Apply the UDF to each partition (sensor), ensuring type safety\n",
    "analyzed_df = partitioned_df.withColumn(\"analysis_result\", analyze_timeseries_udf(col(\"consumed_unit\").cast(\"double\")))\n",
    "\n",
    "# View the results\n",
    "analyzed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_data = df.groupBy(\"sensor_id\")\n",
    "df_repartitoned = df.repartition\n",
    "\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * total_rows)\n",
    "        train_rows = max(0, total_rows - test_rows)\n",
    "        print(total_rows,test_rows,train_rows)\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        return train,test\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "train_test_splits = grouped_data.flatMap(lambda group: train_test_split(group[1]))\n",
    "\n",
    "train_test_splits = grouped_data.apply(train_test_split)\n",
    "\n",
    "# Collect train-test splits\n",
    "train_test_splits = train_test_splits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"creation_time\", StringType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True)])\n",
    "\n",
    "def train_test_split(key, iterator):\n",
    "    df = pd.DataFrame(list(iterator), columns=df.columns)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    yield train_df, test_df\n",
    "\n",
    "# Apply train-test split function to each group in parallel\n",
    "train_test_splits = df.groupBy(\"sensor_id\").applyInPandas(train_test_split, schema)\n",
    "\n",
    "# Collect train-test splits\n",
    "train_test_splits_list = train_test_splits.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of each train-test split DataFrame\n",
    "for train_test_split_df in train_test_splits.take(5):\n",
    "    train_test_split_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file_path = \"schema.yaml\"\n",
    "# Validate the data against the schema\n",
    "validated_df = reading_and_validate_data(spark, data, schema_file_path)\n",
    "\n",
    "# Check if validation was successful\n",
    "if validated_df is not None:\n",
    "    # validated_df.show()\n",
    "    df = validated_df.select(\"creation_time\",\"sensor_id\",\"consumed_unit\")\n",
    "    # df = validated_df\n",
    "    df.show()\n",
    "    df.createOrReplaceTempView(\"data\")\n",
    "    spark.sql(\"\"\"\n",
    "    select sensor_id,sum(consumed_unit) as units, count(sensor_id) as data_count\n",
    "    from data\n",
    "    group by sensor_id\n",
    "    order by data_count desc\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Data validation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "test_size = 24 * 6 * 4  # 7 days * 24 hours/day\n",
    "gap = 24*4  # 1 day * 24 hours/day\n",
    "\n",
    "total_size = test_size + gap\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=3, test_size=total_size, gap=gap)\n",
    "\n",
    "for train_idx, val_idx in tss.split(pandas_df):\n",
    "    train_data = pandas_df.iloc[train_idx]\n",
    "    test_data = pandas_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 18,6\n",
    "plt.plot(test_data[\"creation_time\"],test_data['consumed_unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = sdf.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(count(\"sensor_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select(count(\"sensor_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rows = 7 * 24 * 4\n",
    "\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * total_rows)\n",
    "        train_rows = max(0, total_rows - test_rows)\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        return train,test\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "        \n",
    "def train_test_split_multi_timeseries(df):\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())\n",
    "\n",
    "    # Calculate total and test rows (adjust test_size as needed)\n",
    "    total_rows = df.count()\n",
    "    test_rows = int(0.2 * total_rows)\n",
    "\n",
    "    # Ensure train_rows is not negative\n",
    "    train_rows = max(0, total_rows - test_rows)\n",
    "\n",
    "    # Apply window and filter for training and testing data\n",
    "    df_with_window = df.withColumn(\"window_id\", window_spec)    \n",
    "    train = df_with_window.filter(col(\"window_id\") <= train_rows)\n",
    "    test = df_with_window.filter(col(\"window_id\") > train_rows).orderBy(col(\"creation_time\").desc())\n",
    "    return train, test\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())  # Order by ascending time\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())  # Order by ascending creation time\n",
    "\n",
    "    # Calculate total and test rows (adjust test_size as needed)\n",
    "    total_rows = df.count()\n",
    "    test_rows = int(0.2 * total_rows)\n",
    "\n",
    "    # Ensure train_rows is not negative\n",
    "    train_rows = max(0, total_rows - test_rows)  # Handle potential negative train_rows\n",
    "\n",
    "    # Apply window and filter for training and testing data\n",
    "    df_with_window = df.withColumn(\"window_id\", window_spec)\n",
    "    train = df_with_window.filter(col(\"window_id\") <= train_rows)\n",
    "    test = df_with_window.filter(col(\"window_id\") > train_rows).orderBy(col(\"creation_time\").desc())  # Descending order for test\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the split ratios (e.g., 80% training, 20% testing)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Define a window partitioned by sensor_id and ordered by creation_time\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"creation_time\")\n",
    "\n",
    "# Add row numbers within each partition (sensor_id)\n",
    "sdf_with_row_number = sdf.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Calculate the split row numbers for each sensor group\n",
    "split_row_number = (col(\"max_row_number\") * train_ratio).cast(\"int\")\n",
    "\n",
    "# Split the data into training and testing sets based on row numbers\n",
    "train_data = sdf_with_row_number.filter(col(\"row_number\") <= split_row_number)\n",
    "test_data = sdf_with_row_number.filter(col(\"row_number\") > split_row_number)\n",
    "\n",
    "# Drop the row_number column from the final DataFrames\n",
    "train_data = train_data.drop(\"row_number\")\n",
    "test_data = test_data.drop(\"row_number\")\n",
    "\n",
    "# Show the number of rows in each set\n",
    "print(\"Number of rows in training set:\", train_data.count())\n",
    "print(\"Number of rows in testing set:\", test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split_multi_timeseries(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(count(\"sensor_id\")).show()\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select(count(\"sensor_id\")).show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "\n",
    "\n",
    "def train_test_split(df):\n",
    "    \"\"\"Splits a DataFrame into train and test sets based on time series.\"\"\"\n",
    "\n",
    "    try:\n",
    "        window_spec = Window.orderBy(F.col(\"creation_time\").asc()) \\\n",
    "                           .partitionBy(\"sensor_id\") \\\n",
    "                           .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "        df = df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * sdf.count())\n",
    "        train_rows = total_rows - test_rows\n",
    "\n",
    "        # Ensure train_rows is not negative\n",
    "        if train_rows < 0:\n",
    "            raise ValueError(\"Test rows exceed total rows. Adjust the test size.\")\n",
    "\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        total_rows = df.count()\n",
    "        train_threshold = total_rows - test_rows\n",
    "\n",
    "        train_df = df.where(F.col(\"row_number\") <= train_threshold)\n",
    "        test_df = df.filter(F.col(\"row_number\") > train_threshold)\n",
    "\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(\"Error in splitting data:\", e)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df.copy())  # Make a copy to avoid modifying original DataFrame\n",
    "\n",
    "# Now you can use train_df and test_df for further analysis or machine learning tasks\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count_distinct(\"sensor_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"sensor_id\")\n",
    "# data = sdf.withColumn(\"total_units\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sdf.withColumn(\"total_units\", sum(\"consumed_unit\").over(window_spec)) \\\n",
    "               .withColumn(\"average_units\", avg(\"consumed_unit\").over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf\n",
    "test_data_size = 24 * 4 * 7  # 24 hours * 4 quarters per hour * 7 days\n",
    "\n",
    "# Sort the DataFrame by creation_time\n",
    "sorted_df = df.sort(col(\"creation_time\"))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sorted_df.limit(sorted_df.count() - test_data_size)\n",
    "test_data = sorted_df.limit(test_data_size)\n",
    "\n",
    "# Show the schemas of the train and test DataFrames\n",
    "train_data.printSchema()\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def train_test_split_per_sensor(df, test_size_days=7, gap_days=1):\n",
    "    \"\"\"\n",
    "    Perform train-test split for each sensor_id using window functions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: PySpark DataFrame containing the time series data with columns \"creation_time\" and \"sensor_id\".\n",
    "    - test_size_days: Size of the test set in days.\n",
    "    - gap_days: Gap between test sets in days.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: PySpark DataFrame containing the training data.\n",
    "    - test_data: PySpark DataFrame containing the test data.\n",
    "    \"\"\"\n",
    "    # Define window specification partitioned by sensor_id and ordered by creation_time\n",
    "    window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"creation_time\")\n",
    "\n",
    "    # Calculate test and train timestamps using lag function\n",
    "    df_with_timestamps = df.withColumn(\"test_end_timestamp\", F.lag(\"creation_time\", test_size_days).over(window_spec)) \\\n",
    "                           .withColumn(\"test_start_timestamp\", F.expr(f\"test_end_timestamp - INTERVAL {test_size_days} DAYS\")) \\\n",
    "                           .withColumn(\"train_end_timestamp\", F.expr(f\"test_start_timestamp - INTERVAL {gap_days} DAYS\")) \\\n",
    "                           .withColumn(\"train_start_timestamp\", F.first(\"creation_time\").over(window_spec))\n",
    "\n",
    "    # Perform train-test split based on calculated timestamps\n",
    "    train_data = df_with_timestamps.filter((F.col(\"creation_time\") >= F.col(\"train_start_timestamp\")) & (F.col(\"creation_time\") < F.col(\"train_end_timestamp\")))\n",
    "    test_data = df_with_timestamps.filter((F.col(\"creation_time\") >= F.col(\"test_start_timestamp\")) & (F.col(\"creation_time\") < F.col(\"test_end_timestamp\")))\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Example usage:\n",
    "train_data, test_data = train_test_split_per_sensor(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "test_size_intervals = 7 * 24 * 4  # 7 days * 24 hours/day * 4 intervals/hour (15 minutes)\n",
    "gap_intervals = 24 * 4  # 1 day * 24 hours/day * 4 intervals/hour (15 minutes)\n",
    "\n",
    "# Convert intervals to seconds\n",
    "test_size_seconds = test_size_intervals * 15 * 60\n",
    "gap_seconds = gap_intervals * 15 * 60\n",
    "\n",
    "# Function to perform train-test split for each sensor_id\n",
    "def split_train_test_per_sensor(rows):\n",
    "    for sensor_id, data_per_sensor in rows:\n",
    "        min_timestamp = data_per_sensor.select(F.min(\"creation_time\")).first()[0]\n",
    "        max_timestamp = data_per_sensor.select(F.max(\"creation_time\")).first()[0]\n",
    "        \n",
    "        # Calculate test start and end timestamps\n",
    "        test_end_timestamp = max_timestamp\n",
    "        test_start_timestamp = test_end_timestamp - test_size_seconds\n",
    "        \n",
    "        # Calculate train start and end timestamps\n",
    "        train_end_timestamp = test_start_timestamp - gap_seconds\n",
    "        train_start_timestamp = min_timestamp\n",
    "        \n",
    "        # Perform train-test split for the current sensor_id\n",
    "        train_data = data_per_sensor.filter((F.col(\"creation_time\") >= train_start_timestamp) & (F.col(\"creation_time\") < train_end_timestamp))\n",
    "        test_data = data_per_sensor.filter((F.col(\"creation_time\") >= test_start_timestamp) & (F.col(\"creation_time\") < test_end_timestamp))\n",
    "        \n",
    "        yield sensor_id, train_data, test_data\n",
    "\n",
    "# Perform groupBy operation on sensor_id and apply the split_train_test_per_sensor function\n",
    "train_test_per_sensor = df.groupBy(\"sensor_id\").mapPartitions(split_train_test_per_sensor)\n",
    "\n",
    "# Collect the results if needed\n",
    "results = train_test_per_sensor.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, lit\n",
    "\n",
    "def timeseries_train_test_split_sensor(df, training_ratio, test_size, gap_size):\n",
    "  \"\"\"\n",
    "  This function splits data for each unique sensor_id into training and testing sets \n",
    "  based on timestamps.\n",
    "\n",
    "  Args:\n",
    "      df: PySpark DataFrame containing time series data\n",
    "      training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "      test_size: Number of time steps for the testing set\n",
    "      gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "  Returns:\n",
    "      A PySpark DataFrame containing sensor_id, training data, and testing data\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate training data size based on ratio\n",
    "  training_size = int(df.count() * training_ratio)\n",
    "\n",
    "  # Window function to calculate the training end timestamp\n",
    "  window_spec = window.orderBy(\"timestamp\").partitionBy(\"sensor_id\").rowsBetween(-test_size - gap_size, -1)\n",
    "  df_with_training_end_ts = df.withColumn(\"training_end_ts\", window_spec.end())\n",
    "\n",
    "  # Filter training and testing data based on timestamp\n",
    "  training_data = df.where(col(\"timestamp\") <= col(\"training_end_ts\"))\n",
    "  testing_data = df.where(col(\"timestamp\") > col(\"training_end_ts\"))\n",
    "\n",
    "  # Ensure training data covers training_size rows (optional)\n",
    "  training_data = training_data.limit(training_size)\n",
    "\n",
    "  # Combine sensor_id, training, and testing data into a single DataFrame\n",
    "  split_data = training_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"training\"))\\\n",
    "               .union(testing_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"testing\")))\n",
    "\n",
    "  return split_data\n",
    "\n",
    "# Split data for each sensor\n",
    "split_data = df.groupBy(\"sensor_id\").applyInPartitions(timeseries_train_test_split_sensor,lit(0.8), lit(24 * 7), lit(24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, lit\n",
    "\n",
    "def timeseries_train_test_split_sensor(iterator, training_ratio, test_size, gap_size):\n",
    "    \"\"\"\n",
    "    This function splits data for each unique sensor_id into training and testing sets \n",
    "    based on timestamps (for mapPartitions).\n",
    "\n",
    "    Args:\n",
    "        iterator: Iterator over a partition of the grouped DataFrame\n",
    "        training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "        test_size: Number of time steps for the testing set\n",
    "        gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "    Yields:\n",
    "        Rows containing sensor_id, training data, and testing data\n",
    "    \"\"\"\n",
    "    # Calculate training data size based on ratio\n",
    "    training_size = int(df.count() * training_ratio)\n",
    "\n",
    "    # Window function to calculate the training end timestamp\n",
    "    window_spec = window.orderBy(\"timestamp\").partitionBy(\"sensor_id\").rowsBetween(-test_size - gap_size, -1)\n",
    "    df_with_training_end_ts = df.withColumn(\"training_end_ts\", window_spec.end())\n",
    "\n",
    "    # Filter training and testing data based on timestamp\n",
    "    training_data = df.where(col(\"timestamp\") <= col(\"training_end_ts\"))\n",
    "    testing_data = df.where(col(\"timestamp\") > col(\"training_end_ts\"))\n",
    "\n",
    "    # Ensure training data covers training_size rows (optional)\n",
    "    training_data = training_data.limit(training_size)\n",
    "\n",
    "    # Combine sensor_id, training, and testing data into a single DataFrame\n",
    "    split_data = training_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"training\"))\\\n",
    "                 .union(testing_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"testing\")))\n",
    "\n",
    "    # Yield rows from the combined DataFrame\n",
    "    for row in split_data.rdd.collect():\n",
    "      yield row\n",
    "\n",
    "# Split data for each sensor\n",
    "split_data = df.groupBy(\"sensor_id\").mapPartitions(timeseries_train_test_split_sensor,lit(0.8), lit(24 * 7), lit(24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lit, struct, window\n",
    "\n",
    "def train_test_split_udf(data, training_ratio, test_size, gap_size):\n",
    "  \"\"\"\n",
    "  This UDF splits data for a single sensor into training and testing sets \n",
    "  based on timestamps.\n",
    "\n",
    "  Args:\n",
    "      data: Sub-DataFrame for a single sensor_id\n",
    "      training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "      test_size: Number of time steps for the testing set\n",
    "      gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing two DataFrames (training_data, testing_data)\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate training data size based on ratio\n",
    "  training_size = int(data.count() * training_ratio)\n",
    "\n",
    "  # Order data by timestamp\n",
    "  data = data.orderBy(\"timestamp\")  # Assuming you have a \"timestamp\" column\n",
    "\n",
    "  # Calculate end timestamp for training data (considering gap)\n",
    "  training_end_ts = data.select(window.last(\"timestamp\")).rdd.map(lambda r: r[0]).first() - \\\n",
    "                    lit(test_size + gap_size)\n",
    "\n",
    "  # Filter training and testing data based on timestamp\n",
    "  training_data = data.where(col(\"timestamp\") <= training_end_ts)\n",
    "  testing_data = data.where(col(\"timestamp\") > training_end_ts)\n",
    "\n",
    "  # Ensure training data covers training_size rows (optional)\n",
    "  training_data = training_data.limit(training_size)\n",
    "\n",
    "  return training_data, testing_data\n",
    "\n",
    "# Define UDF\n",
    "split_udf = udf(train_test_split_udf, structType=[training_data.schema, testing_data.schema])\n",
    "\n",
    "# Group by sensor_id and apply UDF\n",
    "split_data = df.groupBy(\"sensor_id\").apply(split_udf, lit(0.8), lit(24 * 7), lit(24))\n",
    "\n",
    "# Extract training and testing DataFrames from the split data\n",
    "split_data = split_data.select(\"sensor_id\", split_udf.alias(\"data\").explode())\n",
    "training_data = split_data.select(\"sensor_id\", \"data.training\")\n",
    "testing_data = split_data.select(\"sensor_id\", \"data.testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_data(spark):\n",
    "    \"\"\"\n",
    "    processing data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path1 = \"D:\\Projects\\Spark\\data.csv\"       \n",
    "        spark_df = spark.read.csv(path1,header= True)\n",
    "        spark_df.show()\n",
    "        spark_df.createOrReplaceTempView(\"data\")\n",
    "        spark.sql(\"\"\"\n",
    "        select count(*)\n",
    "        from data \n",
    "\n",
    "        \"\"\").show()\n",
    "    except Exception as e:\n",
    "        print(\"error in traininig:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = initiate_spark()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)  # Enable eager evaluation\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 20)   # Set max number of rows to display\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumColumns\", 200)  # Set max number of columns to display\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)  # Disable truncation\n",
    "\n",
    "if spark:\n",
    "    try:\n",
    "        # training data\n",
    "        train_data(spark)\n",
    "\n",
    "        # display(df.select('sensor_id'))\n",
    "    finally:\n",
    "        # Terminate SparkSession\n",
    "        # terminate_spark(spark)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
